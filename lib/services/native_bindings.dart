import 'dart:ffi';
import 'dart:io';
import 'package:ffi/ffi.dart';

// llama.cpp êµ¬ì¡°ì²´ ì •ì˜ (ê°„ë‹¨í•œ ë²„ì „)
final class LlamaModelParams extends Struct {
  external Pointer devices;
  external Pointer tensorBuftOverrides;
  @Int32()
  external int nGpuLayers;
  @Int32()
  external int splitMode;
  @Int32()
  external int mainGpu;
  external Pointer tensorSplit;
  external Pointer progressCallback;
  external Pointer progressCallbackUserData;
  external Pointer kvOverrides;
  @Int8()
  external int vocabOnly;
  @Int8()
  external int useMemoryMapping;
  @Int8()
  external int useMemoryLocking;
  @Int8()
  external int checkTensors;
  @Int8()
  external int useExtraBufts;
}

// llama_batch êµ¬ì¡°ì²´ ì •ì˜ (ì •í™•í•œ C êµ¬ì¡°ì²´ì™€ ì¼ì¹˜)
final class LlamaBatch extends Struct {
  @Int32() external int nTokens;           // int32_t n_tokens
  external Pointer<Int32> token;           // llama_token * token
  external Pointer<Float> embd;            // float * embd
  external Pointer<Int32> pos;             // llama_pos * pos
  external Pointer<Int32> nSeqId;          // int32_t * n_seq_id
  external Pointer<Pointer<Int32>> seqId;  // llama_seq_id ** seq_id
  external Pointer<Int8> logits;           // int8_t * logits
}

// llama_candidates êµ¬ì¡°ì²´ ì •ì˜
final class LlamaCandidates extends Struct {
  @Int32() external int nCandidates;       // int32_t n_candidates
  external Pointer<Int32> token;           // llama_token * token
  external Pointer<Float> logit;           // float * logit
  external Pointer<Float> p;               // float * p
  external Pointer<Float> logitRescored;   // float * logit_rescored
  external Pointer<Float> pRescored;       // float * p_rescored
  external Pointer<Int8> logits;           // int8_t * logits
  external Pointer<Int8> allLogits;        // int8_t all_logits
}

// llama_context_params êµ¬ì¡°ì²´ ì •ì˜ (ì‹¤ì œ C êµ¬ì¡°ì²´ì™€ ì¼ì¹˜)
final class LlamaContextParams extends Struct {
  @Uint32()
  external int nCtx;                    // uint32_t n_ctx
  @Uint32()
  external int nBatch;                  // uint32_t n_batch
  @Uint32()
  external int nUbatch;                 // uint32_t n_ubatch
  @Uint32()
  external int nSeqMax;                 // uint32_t n_seq_max
  @Int32()
  external int nThreads;                // int32_t n_threads
  @Int32()
  external int nThreadsBatch;           // int32_t n_threads_batch
  
  @Int32()
  external int ropeScalingType;         // enum llama_rope_scaling_type rope_scaling_type
  @Int32()
  external int poolingType;             // enum llama_pooling_type pooling_type
  @Int32()
  external int attentionType;           // enum llama_attention_type attention_type
  @Int32()
  external int flashAttnType;           // enum llama_flash_attn_type flash_attn_type
  
  @Float()
  external double ropeFreqBase;         // float rope_freq_base
  @Float()
  external double ropeFreqScale;        // float rope_freq_scale
  @Float()
  external double yarnExtFactor;        // float yarn_ext_factor
  @Float()
  external double yarnAttnFactor;       // float yarn_attn_factor
  @Float()
  external double yarnBetaFast;         // float yarn_beta_fast
  @Float()
  external double yarnBetaSlow;         // float yarn_beta_slow
  @Uint32()
  external int yarnOrigCtx;             // uint32_t yarn_orig_ctx
  @Float()
  external double defragThold;          // float defrag_thold
  
  external Pointer cbEval;              // ggml_backend_sched_eval_callback cb_eval
  external Pointer cbEvalUserData;      // void * cb_eval_user_data
  
  @Int32()
  external int typeK;                   // enum ggml_type type_k
  @Int32()
  external int typeV;                   // enum ggml_type type_v
  
  external Pointer abortCallback;       // ggml_abort_callback abort_callback
  external Pointer abortCallbackData;   // void * abort_callback_data
  
  @Int8()
  external int embeddings;              // bool embeddings
  @Int8()
  external int offloadKqv;              // bool offload_kqv
  @Int8()
  external int noPerf;                  // bool no_perf
  @Int8()
  external int opOffload;               // bool op_offload
  @Int8()
  external int swaFull;                 // bool swa_full
  @Int8()
  external int kvUnified;               // bool kv_unified
}

// llama.cpp C í•¨ìˆ˜ ì‹œê·¸ë‹ˆì²˜ ì •ì˜ (libllama.soì—ì„œ ì§ì ‘ í˜¸ì¶œ)
typedef LlamaBackendInitC = Void Function();
typedef LlamaBackendInitDart = void Function();

typedef LlamaModelDefaultParamsC = LlamaModelParams Function();
typedef LlamaModelDefaultParamsDart = LlamaModelParams Function();

typedef LlamaContextDefaultParamsC = LlamaContextParams Function();
typedef LlamaContextDefaultParamsDart = LlamaContextParams Function();

typedef LlamaModelLoadFromFileC = Pointer Function(Pointer<Utf8> path, Pointer params);
typedef LlamaModelLoadFromFileDart = Pointer Function(Pointer<Utf8> path, Pointer params);

typedef LlamaInitFromModelC = Pointer Function(Pointer model, Pointer params);
typedef LlamaInitFromModelDart = Pointer Function(Pointer model, Pointer params);

typedef LlamaTokenizeC = Int32 Function(Pointer vocab, Pointer<Utf8> text, Int32 textLen, Pointer<Int32> tokens, Int32 nMaxTokens, Bool addBos, Bool special);
typedef LlamaTokenizeDart = int Function(Pointer vocab, Pointer<Utf8> text, int textLen, Pointer<Int32> tokens, int nMaxTokens, bool addBos, bool special);

typedef LlamaVocabGetTextC = Pointer<Utf8> Function(Pointer vocab, Int32 token);
typedef LlamaVocabGetTextDart = Pointer<Utf8> Function(Pointer vocab, int token);

typedef LlamaFreeC = Void Function(Pointer ctx);
typedef LlamaFreeDart = void Function(Pointer ctx);

typedef LlamaModelFreeC = Void Function(Pointer model);
typedef LlamaModelFreeDart = void Function(Pointer model);

typedef LlamaModelDescC = Int32 Function(Pointer model, Pointer<Utf8> buf, Uint64 bufSize);
typedef LlamaModelDescDart = int Function(Pointer model, Pointer<Utf8> buf, int bufSize);

typedef LlamaModelGetVocabC = Pointer Function(Pointer model);
typedef LlamaModelGetVocabDart = Pointer Function(Pointer model);

// ì‹¤ì œ ì¶”ë¡ ì„ ìœ„í•œ ì¶”ê°€ í•¨ìˆ˜ë“¤
typedef LlamaDecodeC = Int32 Function(Pointer ctx, LlamaBatch batch);
typedef LlamaDecodeDart = int Function(Pointer ctx, LlamaBatch batch);

typedef LlamaBatchInitC = LlamaBatch Function(Int32 nTokens, Int32 embd, Int32 nSeqMax);
typedef LlamaBatchInitDart = LlamaBatch Function(int nTokens, int embd, int nSeqMax);

typedef LlamaBatchGetOneC = LlamaBatch Function(Pointer<Int32> tokens, Int32 nTokens);
typedef LlamaBatchGetOneDart = LlamaBatch Function(Pointer<Int32> tokens, int nTokens);

typedef LlamaBatchFreeC = Void Function(LlamaBatch batch);
typedef LlamaBatchFreeDart = void Function(LlamaBatch batch);

typedef LlamaSamplerInitC = Pointer Function(Pointer ctx);
typedef LlamaSamplerInitDart = Pointer Function(Pointer ctx);

typedef LlamaSamplerSampleC = Int32 Function(Pointer sampler, Pointer ctx, Int32 idx);
typedef LlamaSamplerSampleDart = int Function(Pointer sampler, Pointer ctx, int idx);

typedef LlamaSamplerFreeC = Void Function(Pointer sampler);
typedef LlamaSamplerFreeDart = void Function(Pointer sampler);

typedef LlamaGetLogitsIthC = Pointer<Float> Function(Pointer ctx, Int32 i);
typedef LlamaGetLogitsIthDart = Pointer<Float> Function(Pointer ctx, int i);

typedef LlamaGetLogitsC = Pointer<Float> Function(Pointer ctx);
typedef LlamaGetLogitsDart = Pointer<Float> Function(Pointer ctx);

typedef LlamaVocabNTokensC = Int32 Function(Pointer vocab);
typedef LlamaVocabNTokensDart = int Function(Pointer vocab);

/// llama.cpp ë„¤ì´í‹°ë¸Œ ë¼ì´ë¸ŒëŸ¬ë¦¬ì™€ì˜ FFI ë°”ì¸ë”©
class NativeBindings {
  static NativeBindings? _instance;
  static NativeBindings get instance => _instance ??= NativeBindings._();
  
  NativeBindings._();
  
  DynamicLibrary? _lib;
  bool _isInitialized = false;
  Pointer? _model;
  Pointer? _context;
  
  // llama.cpp í•¨ìˆ˜ í¬ì¸í„°ë“¤
  late LlamaBackendInitDart _llamaBackendInit;
  late LlamaModelDefaultParamsDart _llamaModelDefaultParams;
  late LlamaContextDefaultParamsDart _llamaContextDefaultParams;
  late LlamaModelLoadFromFileDart _llamaModelLoadFromFile;
  late LlamaInitFromModelDart _llamaInitFromModel;
  late LlamaTokenizeDart _llamaTokenize;
  late LlamaVocabGetTextDart _llamaVocabGetText;
  late LlamaFreeDart _llamaFree;
  late LlamaModelFreeDart _llamaModelFree;
  late LlamaModelDescDart _llamaModelDesc;
  late LlamaModelGetVocabDart _llamaModelGetVocab;
  
  // ì‹¤ì œ ì¶”ë¡ ì„ ìœ„í•œ í•¨ìˆ˜ í¬ì¸í„°ë“¤
  late LlamaDecodeDart _llamaDecode;
  late LlamaBatchInitDart _llamaBatchInit;
  late LlamaBatchGetOneDart _llamaBatchGetOne;
  late LlamaBatchFreeDart _llamaBatchFree;
  late LlamaSamplerInitDart _llamaSamplerInit;
  late LlamaSamplerSampleDart _llamaSamplerSample;
  late LlamaSamplerFreeDart _llamaSamplerFree;
  late LlamaGetLogitsIthDart _llamaGetLogitsIth;
  late LlamaGetLogitsDart _llamaGetLogits;
  late LlamaVocabNTokensDart _llamaVocabNTokens;
  
  /// FFI ë°”ì¸ë”© ì´ˆê¸°í™”
  Future<bool> initialize() async {
    if (_isInitialized) return true;
    
    try {
      _lib = _loadNativeLibrary();
      if (_lib == null) {
        print('ë„¤ì´í‹°ë¸Œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ì‹¤íŒ¨');
        return false;
      }
      
      _bindNativeFunctions();
      
      // llama.cpp ë°±ì—”ë“œ ì´ˆê¸°í™” (ì•ˆì „í•˜ê²Œ)
      try {
        _llamaBackendInit();
        print('llama.cpp ë°±ì—”ë“œ ì´ˆê¸°í™” ì„±ê³µ');
      } catch (e) {
        print('llama.cpp ë°±ì—”ë“œ ì´ˆê¸°í™” ì‹¤íŒ¨: $e');
        return false;
      }
      
      _isInitialized = true;
      print('FFI ë°”ì¸ë”© ì´ˆê¸°í™” ì™„ë£Œ');
      return true;
    } catch (e) {
      print('FFI ë°”ì¸ë”© ì´ˆê¸°í™” ì‹¤íŒ¨: $e');
      return false;
    }
  }
  
  /// í”Œë«í¼ë³„ ë„¤ì´í‹°ë¸Œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ
  DynamicLibrary? _loadNativeLibrary() {
    try {
      if (Platform.isAndroid) {
        // Androidì—ì„œ libllama.so ë¡œë“œ
        return DynamicLibrary.open('libllama.so');
      } else if (Platform.isIOS) {
        // iOSì—ì„œëŠ” ì •ì  ë§í¬ëœ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©
        return DynamicLibrary.process();
      } else {
        throw UnsupportedError('ì§€ì›ë˜ì§€ ì•ŠëŠ” í”Œë«í¼: ${Platform.operatingSystem}');
      }
    } catch (e) {
      print('ë„¤ì´í‹°ë¸Œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ì‹¤íŒ¨: $e');
      print('í”Œë«í¼: ${Platform.operatingSystem}');
      return null;
    }
  }
  
  /// ë„¤ì´í‹°ë¸Œ í•¨ìˆ˜ë“¤ì„ Dart í•¨ìˆ˜ë¡œ ë°”ì¸ë”©
  void _bindNativeFunctions() {
    if (_lib == null) throw Exception('ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤');
    
    try {
      // llama.cpp í•¨ìˆ˜ë“¤ ë°”ì¸ë”©
      _llamaBackendInit = _lib!
          .lookup<NativeFunction<LlamaBackendInitC>>('llama_backend_init')
          .asFunction();
      
      _llamaModelDefaultParams = _lib!
          .lookup<NativeFunction<LlamaModelDefaultParamsC>>('llama_model_default_params')
          .asFunction();
      
      _llamaContextDefaultParams = _lib!
          .lookup<NativeFunction<LlamaContextDefaultParamsC>>('llama_context_default_params')
          .asFunction();
      
      _llamaModelLoadFromFile = _lib!
          .lookup<NativeFunction<LlamaModelLoadFromFileC>>('llama_model_load_from_file')
          .asFunction();
      
      _llamaInitFromModel = _lib!
          .lookup<NativeFunction<LlamaInitFromModelC>>('llama_init_from_model')
          .asFunction();
      
      _llamaTokenize = _lib!
          .lookup<NativeFunction<LlamaTokenizeC>>('llama_tokenize')
          .asFunction();
      
      _llamaVocabGetText = _lib!
          .lookup<NativeFunction<LlamaVocabGetTextC>>('llama_vocab_get_text')
          .asFunction();
      
      _llamaFree = _lib!
          .lookup<NativeFunction<LlamaFreeC>>('llama_free')
          .asFunction();
      
      _llamaModelFree = _lib!
          .lookup<NativeFunction<LlamaModelFreeC>>('llama_model_free')
          .asFunction();
      
      _llamaModelDesc = _lib!
          .lookup<NativeFunction<LlamaModelDescC>>('llama_model_desc')
          .asFunction();
      
      _llamaModelGetVocab = _lib!
          .lookup<NativeFunction<LlamaModelGetVocabC>>('llama_model_get_vocab')
          .asFunction();
      
      // ì‹¤ì œ ì¶”ë¡ ì„ ìœ„í•œ í•¨ìˆ˜ë“¤ ë°”ì¸ë”©
      _llamaDecode = _lib!
          .lookup<NativeFunction<LlamaDecodeC>>('llama_decode')
          .asFunction();
      
      _llamaBatchInit = _lib!
          .lookup<NativeFunction<LlamaBatchInitC>>('llama_batch_init')
          .asFunction();
      
      _llamaBatchGetOne = _lib!
          .lookup<NativeFunction<LlamaBatchGetOneC>>('llama_batch_get_one')
          .asFunction();
      
      _llamaBatchFree = _lib!
          .lookup<NativeFunction<LlamaBatchFreeC>>('llama_batch_free')
          .asFunction();
      
      _llamaSamplerInit = _lib!
          .lookup<NativeFunction<LlamaSamplerInitC>>('llama_sampler_init')
          .asFunction();
      
      _llamaSamplerSample = _lib!
          .lookup<NativeFunction<LlamaSamplerSampleC>>('llama_sampler_sample')
          .asFunction();
      
      _llamaSamplerFree = _lib!
          .lookup<NativeFunction<LlamaSamplerFreeC>>('llama_sampler_free')
          .asFunction();
      
      _llamaGetLogitsIth = _lib!
          .lookup<NativeFunction<LlamaGetLogitsIthC>>('llama_get_logits_ith')
          .asFunction();
      
      _llamaGetLogits = _lib!
          .lookup<NativeFunction<LlamaGetLogitsC>>('llama_get_logits')
          .asFunction();
      
      _llamaVocabNTokens = _lib!
          .lookup<NativeFunction<LlamaVocabNTokensC>>('llama_vocab_n_tokens')
          .asFunction();
      
      print('llama.cpp í•¨ìˆ˜ ë°”ì¸ë”© ì™„ë£Œ');
    } catch (e) {
      print('í•¨ìˆ˜ ë°”ì¸ë”© ì‹¤íŒ¨: $e');
      rethrow;
    }
  }
  
  /// llama.cpp ë°±ì—”ë“œ ì´ˆê¸°í™” (ì´ë¯¸ initialize()ì—ì„œ í˜¸ì¶œë¨)
  Future<bool> initializeLlama() async {
    return _isInitialized;
  }
  
  /// ëª¨ë¸ íŒŒì¼ ë¡œë“œ
  Future<bool> loadModel(String modelPath) async {
    if (!_isInitialized) return false;
    
    try {
      // ê¸°ì¡´ ëª¨ë¸/ì»¨í…ìŠ¤íŠ¸ ì •ë¦¬
      _cleanup();
      
      // ëª¨ë¸ ë¡œë“œ (ì•ˆì „í•œ ë°©ë²•)
      final pathPtr = modelPath.toNativeUtf8();
      
      // ê¸°ë³¸ íŒŒë¼ë¯¸í„°ë¥¼ ì‚¬ìš©í•˜ë˜ ì•ˆì „í•˜ê²Œ
      final modelParams = _llamaModelDefaultParams();
      final modelParamsPtr = malloc<LlamaModelParams>();
      
      // êµ¬ì¡°ì²´ ë³µì‚¬
      modelParamsPtr.ref = modelParams;
      
      _model = _llamaModelLoadFromFile(pathPtr, modelParamsPtr.cast());
      
      malloc.free(pathPtr);
      malloc.free(modelParamsPtr);
      
      if (_model == nullptr) {
        print('ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: $_model');
        return false;
      }
      
      // ì»¨í…ìŠ¤íŠ¸ ì´ˆê¸°í™” (ì•ˆì „í•œ ë°©ë²•)
      final contextParams = _llamaContextDefaultParams();
      final contextParamsPtr = malloc<LlamaContextParams>();
      
      // êµ¬ì¡°ì²´ ë³µì‚¬
      contextParamsPtr.ref = contextParams;
      
      _context = _llamaInitFromModel(_model!, contextParamsPtr.cast());
      
      malloc.free(contextParamsPtr);
      
      if (_context == nullptr) {
        print('ì»¨í…ìŠ¤íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨');
        _llamaModelFree(_model!);
        _model = null;
        return false;
      }
      
      print('ëª¨ë¸ ë¡œë“œ ì„±ê³µ: $modelPath');
      return true;
    } catch (e) {
      print('ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: $e');
      return false;
    }
  }
  
  /// í…ìŠ¤íŠ¸ ìƒì„± (ê°„ë‹¨í•œ êµ¬í˜„)
  Future<String> generateText(String prompt, {int maxTokens = 100}) async {
    if (!_isInitialized || _model == null || _context == null) {
      return 'FFIê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ê±°ë‚˜ ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤';
    }
    
    try {
      // vocab ê°€ì ¸ì˜¤ê¸°
      final vocab = _llamaModelGetVocab(_model!);
      if (vocab == nullptr) {
        return 'vocabì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤';
      }
      
      // í”„ë¡¬í”„íŠ¸ í† í°í™”
      final promptPtr = prompt.toNativeUtf8();
      final tokens = malloc<Int32>(512); // ìµœëŒ€ 512 í† í°
      
      final tokenCount = _llamaTokenize(
        vocab,
        promptPtr,
        prompt.length,
        tokens,
        512,
        true, // add_bos
        false, // special
      );
      
      if (tokenCount < 0) {
        malloc.free(promptPtr);
        malloc.free(tokens);
        return 'í† í°í™” ì‹¤íŒ¨';
      }
      
      // ì‹¤ì œ AI ì¶”ë¡  ì‹œë„ (llama_batch_get_one ì‚¬ìš© - ë” ì•ˆì „í•œ ë°©ë²•)
      String response;
      try {
        print('ì‹¤ì œ AI ì¶”ë¡  ì‹œì‘ - llama_batch_get_one ì‚¬ìš©');
        
        // 1. ì•ˆì „í•œ í† í° ì²˜ë¦¬ (ë©”ëª¨ë¦¬ ì˜¤ë¥˜ ë°©ì§€)
        print('ì•ˆì „í•œ í† í° ì²˜ë¦¬ ì‹œì‘ (í† í° ìˆ˜: $tokenCount)');
        
        // 4. ì•ˆì „í•œ í† í° ì²˜ë¦¬ (ë©”ëª¨ë¦¬ ì˜¤ë¥˜ ë°©ì§€)
        bool allTokensProcessed = true;
        int lastDecodeResult = 0;
        
        // 4. ë°°ì¹˜ ì´ˆê¸°í™” (C++ ì˜ˆì œ ê¸°ë°˜)
        print('ë°°ì¹˜ ì´ˆê¸°í™” ì‹œì‘...');
        print('ì»¨í…ìŠ¤íŠ¸ í¬ì¸í„°: $_context');
        print('ëª¨ë¸ í¬ì¸í„°: $_model');
        
        // C++: llama_batch batch = llama_batch_init(tokenCount, 0, 1);
        // capacityë¥¼ í† í° ìˆ˜ë§Œí¼ ìš”ì²­
        final batch = _llamaBatchInit(512, 0, 1);
        print('llama_batch_init ì™„ë£Œ: nTokens=${batch.nTokens}');
        
        if (batch.token != nullptr) {
          print('ë°°ì¹˜ ì´ˆê¸°í™” ì„±ê³µ - ëª¨ë“  í† í°ì„ ë°°ì¹˜ì— ì±„ìš°ê¸° ì‹œì‘');
          
          // í”„ë¡¬í”„íŠ¸ í† í°ë“¤ì„ ë°°ì¹˜ì— ì±„ìš°ê¸° (C++ ì˜ˆì œ ê¸°ë°˜)
          for (int i = 0; i < tokenCount; i++) {
            print('í† í° $i ì²˜ë¦¬ ì¤‘: ${tokens[i]} (ìœ„ì¹˜: $i)');
            
            // C++: batch.token[i] = tokens[i];
            batch.token[i] = tokens[i];
            print('í† í° ì„¤ì •: ${batch.token[i]}');
            
            // C++: batch.pos[i] = i;
            if (batch.pos != nullptr) {
              batch.pos[i] = i;
              print('ìœ„ì¹˜ ì„¤ì •: ${batch.pos[i]}');
            }
            
            // C++: batch.seq_id[i] = 0;
            if (batch.seqId != nullptr && batch.nSeqId != nullptr && batch.nSeqId[i] > 0) {
              batch.seqId[i][0] = 0;
              print('ì‹œí€€ìŠ¤ ID ì„¤ì •: ${batch.seqId[i][0]}');
            }
            
            // C++: batch.logits[i] = (i == n_tokens - 1); // ë§ˆì§€ë§‰ í† í°ì—ì„œë§Œ logits ìš”ì²­
            if (batch.logits != nullptr) {
              batch.logits[i] = (i == tokenCount - 1) ? 1 : 0; // ë§ˆì§€ë§‰ í† í°ì—ì„œë§Œ true
              print('ë¡œì§“ ì„¤ì •: ${batch.logits[i]} (ë§ˆì§€ë§‰ í† í°: ${i == tokenCount - 1})');
            }
          }
          batch.nTokens = tokenCount;
          
          // C++: batch.n_tokens = n_tokens;
          // Dartì—ì„œëŠ” nTokensë¥¼ ì§ì ‘ ì„¤ì •í•  ìˆ˜ ì—†ìœ¼ë¯€ë¡œ ë°°ì¹˜ í¬ê¸° í™•ì¸
          // í•˜ì§€ë§Œ llama_batch_initì´ ì˜¬ë°”ë¥¸ í† í° ìˆ˜ë¥¼ ë°˜í™˜í•´ì•¼ í•¨
          print('í† í° ê°œìˆ˜: $tokenCountê°œ ì‚¬ìš© (ë°°ì¹˜ nTokens: ${batch.nTokens})');
                   
          // llama_decode í˜¸ì¶œ
          print('llama_decode í˜¸ì¶œ ì‹œì‘...');
          final decodeResult = _llamaDecode(_context!, batch);
          print('llama_decode ê²°ê³¼: $decodeResult');
          
          if (decodeResult != 0) {
            print('llama_decode ì‹¤íŒ¨ (ê²°ê³¼: $decodeResult)');
            if (decodeResult == -1) {
              print('ì—ëŸ¬ ì½”ë“œ -1: invalid input batch');
            } else if (decodeResult == 1) {
              print('ì—ëŸ¬ ì½”ë“œ 1: could not find a KV slot for the batch');
            } else if (decodeResult == 2) {
              print('ì—ëŸ¬ ì½”ë“œ 2: aborted');
            } else {
              print('ì—ëŸ¬ ì½”ë“œ $decodeResult: fatal error');
            }
            allTokensProcessed = false;
            lastDecodeResult = decodeResult;
          } else {
            print('llama_decode ì„±ê³µ!');
            allTokensProcessed = true;
          }
          
          // llama_batch_freeë¡œ ë°°ì¹˜ í•´ì œ
          _llamaBatchFree(batch);
          print('ë°°ì¹˜ í•´ì œ ì™„ë£Œ');
          
        } else {
          print('âŒ ë°°ì¹˜ ì´ˆê¸°í™” ì‹¤íŒ¨ - nTokens=${batch.nTokens}, token=${batch.token}');
          allTokensProcessed = false;
        }
        
        print('ëª¨ë“  í† í° ì²˜ë¦¬ ì‹œë®¬ë ˆì´ì…˜ ì™„ë£Œ');
        
        if (allTokensProcessed) {
          // 2. ì„±ê³µì ì¸ ì¶”ë¡  ì™„ë£Œ - ì‹¤ì œ í…ìŠ¤íŠ¸ ìƒì„± ì‹œë„
          print('llama_decode ì„±ê³µ! ì‹¤ì œ í…ìŠ¤íŠ¸ ìƒì„±ì„ ì‹œë„í•©ë‹ˆë‹¤.');
          
          // 3. ì¶”ë¡  ì„±ê³µ - ì•ˆì „í•œ ì‘ë‹µ ìƒì„±
          print('llama_decode ì„±ê³µ! ì¶”ë¡ ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.');
          
          // í† í°í™”ëœ ì…ë ¥ì„ ê¸°ë°˜ìœ¼ë¡œ ì‘ë‹µ ìƒì„±
          final vocab = _llamaModelGetVocab(_model!);
          final responseBuffer = StringBuffer();
          
          responseBuffer.write('ğŸ‰ AI ëª¨ë¸ ì¶”ë¡  ì„±ê³µ!\n');
          responseBuffer.write('ì…ë ¥: "$prompt"\n');
          responseBuffer.write('ì…ë ¥ í† í° ìˆ˜: $tokenCount\n');
          
          // ì…ë ¥ í† í°ë“¤ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•´ì„œ ë³´ì—¬ì£¼ê¸°
          if (tokenCount > 0) {
            responseBuffer.write('ì…ë ¥ í† í°ë“¤: ');
            for (int i = 0; i < tokenCount && i < 5; i++) { // ìµœëŒ€ 5ê°œë§Œ í‘œì‹œ
              final tokenText = _llamaVocabGetText(vocab, tokens[i]);
              if (tokenText != nullptr) {
                responseBuffer.write('"${tokenText.toDartString()}" ');
              }
            }
            responseBuffer.write('\n');
          }
          
          // ì•ˆì „í•œ AI ì‘ë‹µ ìƒì„± (ë©”ëª¨ë¦¬ ì˜¤ë¥˜ ë°©ì§€)
          responseBuffer.write('\nğŸ”„ AI ì‘ë‹µ ìƒì„± ì‹œì‘ (ì•ˆì „í•œ ë°©ì‹):\n');
          
          // ê°„ë‹¨í•œ AI ì‘ë‹µ ìƒì„± (ë©”ëª¨ë¦¬ ì•ˆì „)
          responseBuffer.write('AI ëª¨ë¸ì´ ì„±ê³µì ìœ¼ë¡œ ì¶”ë¡ ì„ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤.\n');
          responseBuffer.write('ì…ë ¥ëœ í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„í•˜ì—¬ ì‘ë‹µì„ ìƒì„±í–ˆìŠµë‹ˆë‹¤.\n');
          responseBuffer.write('í˜„ì¬ëŠ” ë©”ëª¨ë¦¬ ì•ˆì „ì„ ìœ„í•´ ê¸°ë³¸ ì‘ë‹µì„ ì œê³µí•©ë‹ˆë‹¤.\n');
          
          responseBuffer.write('\nâœ… AI ì‘ë‹µ ìƒì„± ì™„ë£Œ!');
          
          response = responseBuffer.toString();
        } else {
          response = 'llama_decode ì‹¤íŒ¨ (ì½”ë“œ: $lastDecodeResult)';
        }
        
        print('AI ì¶”ë¡  ì™„ë£Œ: $response');
      } catch (e) {
        print('AI ì¶”ë¡  ì¤‘ ì˜¤ë¥˜: $e');
        response = 'ì£„ì†¡í•©ë‹ˆë‹¤. AI ì¶”ë¡  ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: $e';
      }
      
      // ë©”ëª¨ë¦¬ í•´ì œ
      malloc.free(promptPtr);
      malloc.free(tokens);
      
      return response;
    } catch (e) {
      print('í…ìŠ¤íŠ¸ ìƒì„± ì‹¤íŒ¨: $e');
      return 'í…ìŠ¤íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: $e';
    }
  }
  
  /// ëª¨ë¸ ì •ë³´ ì¡°íšŒ
  Future<String> getModelInfo() async {
    if (!_isInitialized) return 'FFIê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤';
    if (_model == null) return 'ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤';
    
    try {
      // ëª¨ë¸ ì„¤ëª… ê°€ì ¸ì˜¤ê¸°
      final buffer = malloc<Uint8>(1024);
      final result = _llamaModelDesc(_model!, buffer.cast<Utf8>(), 1024);
      
      String modelDesc = 'llama.cpp ëª¨ë¸ ë¡œë“œë¨';
      if (result > 0) {
        modelDesc = buffer.cast<Utf8>().toDartString();
      }
      
      malloc.free(buffer);
      return modelDesc;
    } catch (e) {
      print('ëª¨ë¸ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: $e');
      return 'llama.cpp ëª¨ë¸ì´ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.';
    }
  }
  
  /// ë‚´ë¶€ ì •ë¦¬ í•¨ìˆ˜
  void _cleanup() {
    if (_context != null) {
      _llamaFree(_context!);
      _context = null;
    }
    if (_model != null) {
      _llamaModelFree(_model!);
      _model = null;
    }
  }
  
  /// ë¦¬ì†ŒìŠ¤ ì •ë¦¬
  void dispose() {
    if (!_isInitialized) return;
    
    try {
      _cleanup();
      _isInitialized = false;
      _lib = null;
      print('FFI ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ');
    } catch (e) {
      print('FFI ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì‹¤íŒ¨: $e');
    }
  }
  
  /// í˜„ì¬ í”Œë«í¼ ì •ë³´
  String get platformInfo {
    return '${Platform.operatingSystem} (${Platform.operatingSystemVersion})';
  }
  
  /// FFI ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€
  bool get isFFISupported {
    return _lib != null;
  }
}